commit d6d196bc0682280a3548a914c7646d59b3771f17
Author: Szymon Piotr Krasuski <krasuski.szymon.piotr@gmail.com>
Date:   Tue Apr 28 09:52:04 2020 +0200

    [stable/distributed-tensorflow] Refactor chart and add logging feature (#21159)
    
    * Refactored distributed-tensorflow chart
    
    Signed-off-by: Szymon <krasuski.szymon.piotr@gmail.com>
    
    * Changes according to nvtkaszpir
    
    Signed-off-by: Szymon <krasuski.szymon.piotr@gmail.com>

diff --git a/stable/distributed-tensorflow/Chart.yaml b/stable/distributed-tensorflow/Chart.yaml
index cd92da6f4..3ab882e46 100644
--- a/stable/distributed-tensorflow/Chart.yaml
+++ b/stable/distributed-tensorflow/Chart.yaml
@@ -1,12 +1,12 @@
 apiVersion: v1
 description: A Helm chart for running distributed TensorFlow on Kubernetes
 name: distributed-tensorflow
-version: 0.1.3
-appVersion: 1.6.0
+version: 1.2.0
+appVersion: 1.7.0
 sources:
 - https://github.com/tensorflow/tensorflow
 - https://www.tensorflow.org/deploy/distributed
-home: https://www.tensorflow.org
+home: https://github.com/helm/charts/tree/master/stable/distributed-tensorflow
 maintainers:
 - name: Dysproz
   email: krasuski.szymon.piotr@gmail.com
diff --git a/stable/distributed-tensorflow/README.md b/stable/distributed-tensorflow/README.md
index 0032f682e..4d22aced4 100644
--- a/stable/distributed-tensorflow/README.md
+++ b/stable/distributed-tensorflow/README.md
@@ -1,6 +1,6 @@
 # Distributed TensorFlow
 
-TensorFlow is an open source software library for numerical computation using data flow graphs, and it supports distributed computing, allowing 'data parallel' or 'model parallel' on different servers. This means data scientists can now scale out distributed training to 100s of GPUs using TensorFlow. 
+TensorFlow is an open source software library for numerical computation using data flow graphs, and it supports distributed computing, allowing 'data parallel' or 'model parallel' on different servers. This means data scientists can now scale out distributed training to 100s of GPUs using TensorFlow.
 
 -  https://www.tensorflow.org
 -  https://www.tensorflow.org/deploy/distributed
@@ -9,24 +9,24 @@ TensorFlow is an open source software library for numerical computation using da
 
 ## Prerequisites
 
-- Kubernetes cluster v1.8+ 
+- Kubernetes cluster v1.12+
 
 ## Chart Details
 
-This chart will create a TensorFlow cluster, and distribute a computation graph across that cluster. 
+This chart will create a TensorFlow cluster, and distribute a computation graph across that cluster.
 
 ## Installing the Chart
 
 * To install the chart with the release name `mnist`:
 
   ```bash
-  $ helm install --name mnist incubator/distributed-tensorflow
+  $ helm install mnist incubator/distributed-tensorflow
   ```
 
 * To install with custom values via file:
 
   ```
-  $ helm install  --values values.yaml  --name mnist  incubator/distributed-tensorflow
+  $ helm install --values values.yaml mnist incubator/distributed-tensorflow
   ```
 
   Below is an example of the custom value file values.yaml with GPU support.
@@ -36,7 +36,7 @@ This chart will create a TensorFlow cluster, and distribute a computation graph
     number: 2
     podManagementPolicy: Parallel
     image:
-      repository: cheyang/distributed-tf
+      repository: dysproz/distributed-tf
       tag: 1.6.0-gpu
     port: 9090
     gpuCount: 1
@@ -45,7 +45,7 @@ This chart will create a TensorFlow cluster, and distribute a computation graph
     number: 2
     podManagementPolicy: Parallel
     image:
-      repository: cheyang/distributed-tf
+      repository: dysproz/distributed-tf
       tag: 1.6.0
       pullPolicy: IfNotPresent
     port: 8080
@@ -57,7 +57,7 @@ This chart will create a TensorFlow cluster, and distribute a computation graph
     trainsteps: 10000
   ```
 
-> Notice: you can check the details of docker image from [Docker hub](https://hub.docker.com/r/cheyang/distributed-tf/)
+> Notice: you can check the details of docker image from [Docker hub](https://hub.docker.com/r/dysproz/distributed-tf/)
 
 ## Uninstalling the Chart
 
@@ -76,11 +76,20 @@ chart and their default values.
 
 | Parameter | Description | Default |
 |-----------|-------------|---------|
-| `worker.image.repository` | TensorFlow Worker Server's image repository | `cheyang/distributed-tf` |
+| `worker.image.repository` | TensorFlow Worker Server's image repository | `dysproz/distributed-tf` |
 | `worker.image.tag` | TensorFlow Worker Server's image tag | `gpu` |
 | `worker.image.pullPolicy` | image pullPolicy for the  worker | `IfNotPresent` |
 | `worker.gpuCount` | Set the gpu to be allocated and allowed for the Pods | `0` |
-| `ps.image.repository` | TensorFlow Parameter Server's image repository | `cheyang/distributed-tf` |
-| `ps.image.tag` | TensorFlow Parameter Server's image tag | `1.6.0-gpu` |
+| `worker.env` | key-value environment variables | None |
+| `ps.image.repository` | TensorFlow Parameter Server's image repository | `dysproz/distributed-tf` |
+| `ps.image.tag` | TensorFlow Parameter Server's image tag | `1.7.0-gpu` |
 | `ps.image.pullPolicy` | image pullPolicy for the  ps | `IfNotPresent` |
-
+| `ps.env` | key-value environment variables | None |
+| `volumes` | List of volumes defined in cluster (in standard k8s format) | host path to */tmp/mnist* |
+| `volumeMounts` | Volumes mounted into Pods (in standard k8s format) | host path volume mounted into */tmp/mnist-log* |
+| `hyperparams.batchsize` | batch size | `20` |
+| `hyperparams.learningrate` | learning rate | `0.001` |
+| `hyperparams.trainsteps` | train steps | `0` (continuous run) |
+| `hyperparams.datadir` | data directory | None |
+| `hyperparams.logdir` | logging directory | None |
+| `hyperparams.hiddenunits` | Hidden units in neural network | None |
diff --git a/stable/distributed-tensorflow/templates/statefulset-ps.yaml b/stable/distributed-tensorflow/templates/statefulset-ps.yaml
index 3b07c01c7..bf74536bb 100644
--- a/stable/distributed-tensorflow/templates/statefulset-ps.yaml
+++ b/stable/distributed-tensorflow/templates/statefulset-ps.yaml
@@ -1,8 +1,4 @@
-{{- $lr := .Values.hyperparams.learningrate -}}
-{{- $batchsize := .Values.hyperparams.batchsize -}}
-{{- $trainsteps := .Values.hyperparams.trainsteps -}}
-
-apiVersion: apps/v1beta2
+apiVersion: apps/v1
 kind: StatefulSet
 metadata:
   name: {{ .Release.Name }}-ps
@@ -41,12 +37,30 @@ spec:
         command:
         - "python"
         - "train_distributed.py"
+{{- if .Values.hyperparams.learningrate }}
         - --learning_rate
-        - {{ $lr | quote }}
+        - "{{ .Values.hyperparams.learningrate }}"
+{{- end }}
+{{- if .Values.hyperparams.batchsize }}
         - --batch_size
-        - {{ $batchsize | quote }}
+        - "{{ .Values.hyperparams.batchsize }}"
+{{- end }}
+{{- if .Values.hyperparams.trainsteps }}
         - --train_steps
-        - {{ $trainsteps | quote }}
+        - "{{ .Values.hyperparams.trainsteps }}"
+{{- end }}
+{{- if .Values.hyperparams.datadir }}
+        - --data_dir
+        - "{{ .Values.hyperparams.datadir }}"
+{{- end }}
+{{- if .Values.hyperparams.logdir }}
+        - --log_dir
+        - "{{ .Values.hyperparams.logdir }}"
+{{- end }}
+{{- if .Values.hyperparams.hiddenunits }}
+        - --hidden_units
+        - "{{ .Values.hyperparams.hiddenunits }}"
+{{- end }}
         env:
         - name: WORKER_HOSTS
           valueFrom:
@@ -83,4 +97,4 @@ spec:
 {{- if .Values.ps.resources }}
         resources:
 {{ toYaml .Values.ps.resources | indent 10 }}
-{{- end }}
\ No newline at end of file
+{{- end }}
diff --git a/stable/distributed-tensorflow/templates/statefulset-worker.yaml b/stable/distributed-tensorflow/templates/statefulset-worker.yaml
index 8ba67d619..0278249ce 100644
--- a/stable/distributed-tensorflow/templates/statefulset-worker.yaml
+++ b/stable/distributed-tensorflow/templates/statefulset-worker.yaml
@@ -1,8 +1,4 @@
-{{- $lr := .Values.hyperparams.learningrate -}}
-{{- $batchsize := .Values.hyperparams.batchsize -}}
-{{- $trainsteps := .Values.hyperparams.trainsteps -}}
-
-apiVersion: apps/v1beta2
+apiVersion: apps/v1
 kind: StatefulSet
 metadata:
   name: {{ .Release.Name }}-worker
@@ -45,12 +41,30 @@ spec:
         - --num_gpus
         - "{{ .Values.worker.gpuCount }}"
 {{- end }}
+{{- if .Values.hyperparams.learningrate }}
         - --learning_rate
-        - {{ $lr | quote }}
+        - "{{ .Values.hyperparams.learningrate }}"
+{{- end }}
+{{- if .Values.hyperparams.batchsize }}
         - --batch_size
-        - {{ $batchsize | quote }}
+        - "{{ .Values.hyperparams.batchsize }}"
+{{- end }}
+{{- if .Values.hyperparams.trainsteps }}
         - --train_steps
-        - {{ $trainsteps | quote }}
+        - "{{ .Values.hyperparams.trainsteps }}"
+{{- end }}
+{{- if .Values.hyperparams.datadir }}
+        - --data_dir
+        - "{{ .Values.hyperparams.datadir }}"
+{{- end }}
+{{- if .Values.hyperparams.logdir }}
+        - --log_dir
+        - "{{ .Values.hyperparams.logdir }}"
+{{- end }}
+{{- if .Values.hyperparams.hiddenunits }}
+        - --hidden_units
+        - "{{ .Values.hyperparams.hiddenunits }}"
+{{- end }}
         env:
         - name: WORKER_HOSTS
           valueFrom:
@@ -78,7 +92,7 @@ spec:
         - containerPort: {{ .Values.worker.port }}
 {{- if .Values.volumeMounts }}
         volumeMounts:
-{{ toYaml .Values.volumeMounts | indent 8 }}
+{{ toYaml .Values.volumeMounts | indent 10 }}
 {{- end }}
 {{- if gt (int .Values.worker.gpuCount) 0 }}
         resources:
diff --git a/stable/distributed-tensorflow/values.yaml b/stable/distributed-tensorflow/values.yaml
index c128a6d97..8abdf2a28 100644
--- a/stable/distributed-tensorflow/values.yaml
+++ b/stable/distributed-tensorflow/values.yaml
@@ -5,17 +5,16 @@ worker:
   number: 2
   podManagementPolicy: Parallel
   image:
-    repository: cheyang/distributed-tf
-    tag: 1.6.0
+    repository: dysproz/distributed-tf
+    tag: 1.7.0
     pullPolicy: IfNotPresent
   port: 9000
-  # gpuCount: 2
 ps:
   number: 2
   podManagementPolicy: Parallel
   image:
-    repository: cheyang/distributed-tf
-    tag: 1.6.0
+    repository: dysproz/distributed-tf
+    tag: 1.7.0
     pullPolicy: IfNotPresent
   port: 8000
 # optimize for training
